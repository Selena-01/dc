import numpy as np
from matplotlib import pyplot as plt
from sklearn.datasets import make_regression

# Generate sample regression data
X, y = make_regression(n_samples=100, n_features=1, noise=20, random_state=42)
X = X.reshape(-1, 1)

# Add bias term column of 1s â†’ X_b = [1, X]
X_b = np.c_[np.ones((100, 1)), X]

# Initialize theta (weights) randomly
theta = np.random.randn(2, 1)

# Hyperparameters
lr = 0.01
iterations = 1000
m = len(X)

for _ in range(iterations):
    y_pred = X_b.dot(theta)
    grad = (2/m) * X_b.T.dot(y_pred - y.reshape(-1, 1))

    # Correct update rule: theta -= lr * gradient
    theta -= lr * grad

# Predictions for final line
y_pred_final = X_b.dot(theta)

# Plotting
plt.figure(figsize=(10, 5))
plt.scatter(X, y, label="Actual Data")
plt.plot(X, y_pred_final,color='red', label="Gradient Descent Prediction")
plt.xlabel("Feature")
plt.ylabel("Target")
plt.title("Linear Regression with Gradient Descent")
plt.legend()
plt.show()
